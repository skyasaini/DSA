{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9680a2",
   "metadata": {},
   "source": [
    "**1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4bc230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "def read_hadoop_config(config_file):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "\n",
    "    if 'core-site' in config:\n",
    "        core_components = config['core-site'].get('fs.defaultFS', '').split('://')\n",
    "        if len(core_components) > 1:\n",
    "            print(\"Core Components of Hadoop:\")\n",
    "            print(\" - NameNode:\", core_components[1])\n",
    "\n",
    "    if 'hdfs-site' in config:\n",
    "        hdfs_components = config['hdfs-site'].get('dfs.namenode.secondary.http-address', '').split(':')\n",
    "        if len(hdfs_components) > 1:\n",
    "            print(\" - SecondaryNameNode:\", hdfs_components[0])\n",
    "\n",
    "    if 'yarn-site' in config:\n",
    "        yarn_components = config['yarn-site'].get('yarn.resourcemanager.hostname', '')\n",
    "        if yarn_components:\n",
    "            print(\" - ResourceManager:\", yarn_components)\n",
    "\n",
    "        if config['yarn-site'].getboolean('yarn.nodemanager.aux-services'):\n",
    "            print(\" - NodeManager\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config_file = '/path/to/hadoop.conf'  # Specify the path to your Hadoop configuration file\n",
    "    read_hadoop_config(config_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b7be4",
   "metadata": {},
   "source": [
    "**2.Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9e5cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snakebite.client import Client\n",
    "\n",
    "def calculate_hdfs_directory_size(directory):\n",
    "    client = Client('localhost', 9000)  # Replace with your HDFS NameNode host and port\n",
    "\n",
    "    total_size = 0\n",
    "    for file in client.ls([directory]):\n",
    "        total_size += file['length']\n",
    "\n",
    "    return total_size\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hdfs_directory = '/user/hadoop/data'  # Specify the HDFS directory path\n",
    "    size = calculate_hdfs_directory_size(hdfs_directory)\n",
    "    print(f\"Total file size in HDFS directory '{hdfs_directory}': {size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f986d",
   "metadata": {},
   "source": [
    "**3.Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9afaa2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def find_top_n_frequent_word(file,N):\n",
    "    # Open the file in read mode\n",
    "    text = open(file,\"r\")\n",
    "    # Create an empty dictionary\n",
    "    d = dict()\n",
    "    \n",
    "    for line in text:\n",
    "        # Remove the leading spaces and newline character\n",
    "        line = line.strip()\n",
    "        # Convert the characters in line to\n",
    "        # lowercase to avoid case mismatch\n",
    "        line.lower()\n",
    "        # Remove the punctuation marks from the line\n",
    "        line = line.translate(line.maketrans(\"\", \"\", string.punctuation))\n",
    "  \n",
    "        # Split the line into words\n",
    "        words = line.split(\" \")\n",
    "  \n",
    "        # Iterate over each word in line\n",
    "        for word in words:\n",
    "            # Check if the word is already in dictionary\n",
    "            if word in d:\n",
    "                # Increment count of word by 1\n",
    "                d[word] = d[word] + 1\n",
    "            else:\n",
    "                # Add the word to dictionary with count 1\n",
    "                d[word] = 1\n",
    "    result = dict(sorted(test_dict.items(), key = lambda x: x[1], reverse = True)[:N])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e660ea7b",
   "metadata": {},
   "source": [
    "#### 4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae09542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_hadoop_cluster_health():\n",
    "    # Hadoop NameNode REST API endpoint\n",
    "    namenode_url = 'http://<namenode-host>:50070/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo'\n",
    "\n",
    "    # Hadoop DataNode REST API endpoint\n",
    "    datanode_url = 'http://<datanode-host>:50075/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo'\n",
    "\n",
    "    # Check NameNode health status\n",
    "    namenode_response = requests.get(namenode_url)\n",
    "    namenode_status = namenode_response.json()['beans'][0]['State']\n",
    "\n",
    "    # Check DataNode health status\n",
    "    datanode_response = requests.get(datanode_url)\n",
    "    datanode_status = datanode_response.json()['beans'][0]['State']\n",
    "\n",
    "    # Print health status\n",
    "    print(f\"NameNode Health: {namenode_status}\")\n",
    "    print(f\"DataNode Health: {datanode_status}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_hadoop_cluster_health()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0760622",
   "metadata": {},
   "source": [
    "#### 5.  Develop a Python program that lists all the files and directories in a specific HDFS path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fda4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snakebite.client import Client\n",
    "\n",
    "def list_hdfs_path(path):\n",
    "    client = Client('localhost', 9000)  # Replace with your HDFS NameNode host and port\n",
    "\n",
    "    for file in client.ls([path]):\n",
    "        print(file['path'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hdfs_path = '/user/hadoop/data'  # Specify the HDFS path\n",
    "    list_hdfs_path(hdfs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611bdc59",
   "metadata": {},
   "source": [
    "#### 6.Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76341c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def analyze_data_node_storage_utilization():\n",
    "    # Hadoop DataNodes REST API endpoint\n",
    "    datanodes_url = 'http://<namenode-host>:50070/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState-*'\n",
    "\n",
    "    # Send a GET request to fetch DataNodes information\n",
    "    response = requests.get(datanodes_url)\n",
    "    datanodes = response.json()['beans']\n",
    "\n",
    "    # Variables to track highest and lowest storage capacities\n",
    "    highest_storage = -1\n",
    "    lowest_storage = float('inf')\n",
    "    highest_storage_datanode = ''\n",
    "    lowest_storage_datanode = ''\n",
    "\n",
    "    # Analyze storage utilization for each DataNode\n",
    "    for datanode in datanodes:\n",
    "        datanode_info = datanode['FSDatasetState']\n",
    "        storage_capacity = datanode_info['Capacity']\n",
    "        storage_used = datanode_info['DfsUsed']\n",
    "\n",
    "        if storage_capacity > highest_storage:\n",
    "            highest_storage = storage_capacity\n",
    "            highest_storage_datanode = datanode_info['DatanodeInfo']['DatanodeID']\n",
    "\n",
    "        if storage_capacity < lowest_storage:\n",
    "            lowest_storage = storage_capacity\n",
    "            lowest_storage_datanode = datanode_info['DatanodeInfo']['DatanodeID']\n",
    "\n",
    "    # Print the results\n",
    "    print(\"DataNode Storage Utilization Analysis:\")\n",
    "    print(f\"Highest Storage Capacity: {highest_storage} bytes ({highest_storage_datanode})\")\n",
    "    print(f\"Lowest Storage Capacity: {lowest_storage} bytes ({lowest_storage_datanode})\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analyze_data_node_storage_utilization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8834a872",
   "metadata": {},
   "source": [
    "#### 7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00bc00c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(jar_path, main_class, input_path, output_path):\n",
    "    # ResourceManager API endpoint to submit a Hadoop job\n",
    "    submit_job_url = 'http://<resourcemanager-host>:8088/ws/v1/cluster/apps/new-application'\n",
    "\n",
    "    # Send a POST request to submit the job\n",
    "    response = requests.post(submit_job_url)\n",
    "    if response.status_code == 200:\n",
    "        application_id = response.json()['application-id']\n",
    "        print(f\"Hadoop job submitted. Application ID: {application_id}\")\n",
    "\n",
    "        # ResourceManager API endpoint to submit the job request\n",
    "        submit_request_url = f'http://<resourcemanager-host>:8088/ws/v1/cluster/apps/{application_id}/app'\n",
    "\n",
    "        # Define the Hadoop job parameters\n",
    "        data = {\n",
    "            \"application-id\": application_id,\n",
    "            \"application-name\": \"Hadoop Job\",\n",
    "            \"am-container-spec\": {\n",
    "                \"commands\": {\n",
    "                    \"command\": f\"hadoop jar {jar_path} {main_class} {input_path} {output_path}\"\n",
    "                },\n",
    "                \"memory\": 1024,\n",
    "                \"vcores\": 1\n",
    "            },\n",
    "            \"unmanaged-AM\": False,\n",
    "            \"max-app-attempts\": 2,\n",
    "            \"resource\": {\n",
    "                \"memory\": 1024,\n",
    "                \"vcores\": 1\n",
    "            },\n",
    "            \"priority\": 0,\n",
    "            \"queue\": \"default\",\n",
    "            \"timeout\": 0,\n",
    "            \"attempt-failures-validity-interval\": -1,\n",
    "            \"keep-containers-across-application-attempts\": False,\n",
    "            \"application-type\": \"MAPREDUCE\"\n",
    "        }\n",
    "\n",
    "        # Send a PUT request to submit the job request\n",
    "        response = requests.put(submit_request_url, json=data)\n",
    "        if response.status_code == 202:\n",
    "            print(\"Hadoop job request submitted successfully.\")\n",
    "\n",
    "            # Monitor job progress\n",
    "            while True:\n",
    "                # ResourceManager API endpoint to get job status\n",
    "                job_status_url = f'http://<resourcemanager-host>:8088/ws/v1/cluster/apps/{application_id}'\n",
    "\n",
    "                # Send a GET request to get job status\n",
    "                response = requests.get(job_status_url)\n",
    "                if response.status_code == 200:\n",
    "                    job_info = response.json()['app']\n",
    "                    if job_info['state'] == 'FINISHED':\n",
    "                        print(\"Hadoop job completed.\")\n",
    "                        break\n",
    "\n",
    "                    progress = job_info['progress']\n",
    "                    print(f\"Job progress: {progress}\")\n",
    "\n",
    "                time.sleep(5)  # Wait for 5 seconds before checking job status\n",
    "\n",
    "        else:\n",
    "            print(\"Failed to submit the Hadoop job request.\")\n",
    "    else:\n",
    "        print(\"Failed to submit the Hadoop job.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    jar_path = '/path/to/hadoop-job.jar'  # Specify the path to your Hadoop job JAR file\n",
    "    main_class = 'com.example.hadoop.JobMainClass'  # Specify the main class of your Hadoop job\n",
    "    input_path = '/user/hadoop/input'  # Specify the HDFS input path\n",
    "    output_path = '/user/hadoop/output'  # Specify the HDFS output path\n",
    "\n",
    "    submit_hadoop_job(jar_path, main_class, input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0872f7",
   "metadata": {},
   "source": [
    "#### 8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90f4883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(jar_path, main_class, input_path, output_path, memory, vcores):\n",
    "    # ResourceManager API endpoint to submit a Hadoop job\n",
    "    submit_job_url = 'http://<resourcemanager-host>:8088/ws/v1/cluster/apps/new-application'\n",
    "\n",
    "    # Send a POST request to submit the job\n",
    "    response = requests.post(submit_job_url)\n",
    "    if response.status_code == 200:\n",
    "        application_id = response.json()['application-id']\n",
    "        print(f\"Hadoop job submitted. Application ID: {application_id}\")\n",
    "\n",
    "        # ResourceManager API endpoint to submit the job request\n",
    "        submit_request_url = f'http://<resourcemanager-host>:8088/ws/v1/cluster/apps/{application_id}/app'\n",
    "\n",
    "        # Define the Hadoop job parameters\n",
    "        data = {\n",
    "            \"application-id\": application_id,\n",
    "            \"application-name\": \"Hadoop Job\",\n",
    "            \"am-container-spec\": {\n",
    "                \"commands\": {\n",
    "                    \"command\": f\"hadoop jar {jar_path} {main_class} {input_path} {output_path}\"\n",
    "                },\n",
    "                \"resource\": {\n",
    "                    \"memory\": memory,\n",
    "                    \"vcores\": vcores\n",
    "                }\n",
    "            },\n",
    "            \"resource\": {\n",
    "                \"memory\": memory,\n",
    "                \"vcores\": vcores\n",
    "            },\n",
    "            \"application-type\": \"MAPREDUCE\"\n",
    "        }\n",
    "\n",
    "        # Send a PUT request to submit the job request\n",
    "        response = requests.put(submit_request_url, json=data)\n",
    "        if response.status_code == 202:\n",
    "            print(\"Hadoop job request submitted successfully.\")\n",
    "\n",
    "            # ResourceManager API endpoint to get job status\n",
    "            job_status_url = f'http://<resourcemanager-host>:8088/ws/v1/cluster/apps/{application_id}'\n",
    "\n",
    "            # Monitor job progress and resource usage\n",
    "            while True:\n",
    "                # Send a GET request to get job status\n",
    "                response = requests.get(job_status_url)\n",
    "                if response.status_code == 200:\n",
    "                    job_info = response.json()['app']\n",
    "                    if job_info['state'] == 'FINISHED':\n",
    "                        print(\"Hadoop job completed.\")\n",
    "                        break\n",
    "\n",
    "                    progress = job_info['progress']\n",
    "                    allocated_memory = job_info['allocatedMB']\n",
    "                    allocated_vcores = job_info['allocatedVCores']\n",
    "                    print(f\"Job progress: {progress}\")\n",
    "                    print(f\"Allocated resources: Memory = {allocated_memory} MB, vCores = {allocated_vcores}\")\n",
    "\n",
    "                time.sleep(5)  # Wait for 5 seconds before checking job status\n",
    "\n",
    "        else:\n",
    "            print(\"Failed to submit the Hadoop job request.\")\n",
    "    else:\n",
    "        print(\"Failed to submit the Hadoop job.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    jar_path = '/path/to/hadoop-job.jar'  # Specify the path to your Hadoop job JAR file\n",
    "    main_class = 'com.example.hadoop.JobMainClass'  # Specify the main class of your Hadoop job\n",
    "    input_path = '/user/hadoop/input'  # Specify the HDFS input path\n",
    "    output_path = '/user/hadoop/output'  # Specify the HDFS output path\n",
    "    memory = 2048  # Specify the required memory in MB\n",
    "    vcores = 2  # Specify the required number of vCores\n",
    "\n",
    "    submit_hadoop_job(jar_path, main_class, input_path, output_path, memory, vcores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e5793",
   "metadata": {},
   "source": [
    "#### 9.Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edeecc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "import time\n",
    "\n",
    "class MapReduceJob(MRJob):\n",
    "    \n",
    "    def configure_args(self):\n",
    "        super(MapReduceJob, self).configure_args()\n",
    "        self.add_passthru_arg('--split-size', type=int, help='Input split size in bytes')\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        yield None, len(line)\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    split_sizes = [10, 100, 1000]  # Specify the different input split sizes in bytes\n",
    "\n",
    "    for split_size in split_sizes:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Run the MapReduce job with the specified input split size\n",
    "        job = MapReduceJob(args=['-r', 'local', '--split-size', str(split_size), 'input.txt'])\n",
    "        with job.make_runner() as runner:\n",
    "            runner.run()\n",
    "\n",
    "            # Print the job execution time\n",
    "            execution_time = time.time() - start_time\n",
    "            print(f\"Input Split Size: {split_size} bytes\")\n",
    "            print(f\"Job Execution Time: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d588a2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
